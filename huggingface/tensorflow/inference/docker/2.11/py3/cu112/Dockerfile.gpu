# https://github.com/aws/deep-learning-containers/blob/master/available_images.md
# refer to the above page to pull latest Tensorflow image

# docker image region us-west-2
FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.11.0-gpu-py39-cu112-ubuntu20.04-sagemaker

LABEL maintainer="Amazon AI"
LABEL dlc_major_version="1"

# Specify accept-bind-to-port LABEL for inference pipelines to use SAGEMAKER_BIND_TO_PORT
# https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-real-time.html
LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true

ARG PYTHON=python3.9
ARG MMS_VERSION=1.1.8
ARG OPEN_MPI_VERSION=4.1.4
# HF ARGS
ARG TF_URL=https://framework-binaries.s3.us-west-2.amazonaws.com/tensorflow/r2.11_aws/gpu/2022-12-22-20-41/tensorflow_gpu-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
ARG TRANSFORMERS_VERSION

RUN apt-get update \
 # TODO: Remove upgrade statements once packages are updated in base image
 && apt-get -y upgrade --only-upgrade systemd openssl cryptsetup \
 && apt-get install -y --no-install-recommends \
    openjdk-11-jdk-headless \
    libsndfile1-dev \
    ffmpeg \
 && apt-get clean \
 && rm -rf /var/lib/apt/lists/*

RUN wget --quiet https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-${OPEN_MPI_VERSION}.tar.gz \
 && gunzip -c openmpi-${OPEN_MPI_VERSION}.tar.gz | tar xf - \
 && cd openmpi-${OPEN_MPI_VERSION} \
 && ./configure --prefix=/home/.openmpi --with-cuda \
 && make all install \
 && cd .. \
 && rm openmpi-${OPEN_MPI_VERSION}.tar.gz \
 && rm -rf openmpi-${OPEN_MPI_VERSION}

ENV PATH="$PATH:/home/.openmpi/bin"
ENV LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/home/.openmpi/lib/"

WORKDIR /

RUN pip install --no-cache-dir \
    multi-model-server==$MMS_VERSION \
    sagemaker-inference

RUN useradd -m model-server \
 && mkdir -p /home/model-server/tmp \
 && chown -R model-server /home/model-server

COPY mms-entrypoint.py /usr/local/bin/dockerd-entrypoint.py
COPY config.properties /etc/sagemaker-mms.properties

RUN chmod +x /usr/local/bin/dockerd-entrypoint.py

#################################
# Hugging Face specific section #
#################################

# Install TF Binary
RUN pip install --no-cache-dir -U ${TF_URL}

# We need TF_FORCE_GPU_ALLOW_GROWTH=true to prevent TF from overusing GPU memory when loading models
ENV TF_FORCE_GPU_ALLOW_GROWTH=true

# install Hugging Face libraries and its dependencies
RUN pip install --no-cache-dir \
    transformers[sentencepiece,audio,vision]==${TRANSFORMERS_VERSION} \
    "protobuf>=3.19.5,<3.20" \
    "numpy<1.24,>=1.18" \
    # Putting a cap in versions number to avoid potential issues with a new major version
    "urllib3>=1.25.9" \
    "sagemaker-huggingface-inference-toolkit<3"

RUN HOME_DIR=/root \
 && curl -o ${HOME_DIR}/oss_compliance.zip https://aws-dlinfra-utilities.s3.amazonaws.com/oss_compliance.zip \
 && unzip ${HOME_DIR}/oss_compliance.zip -d ${HOME_DIR}/ \
 && cp ${HOME_DIR}/oss_compliance/test/testOSSCompliance /usr/local/bin/testOSSCompliance \
 && chmod +x /usr/local/bin/testOSSCompliance \
 && chmod +x ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh \
 && ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh ${HOME_DIR} ${PYTHON} \
 && rm -rf ${HOME_DIR}/oss_compliance*

EXPOSE 8080 8081
ENTRYPOINT ["python", "/usr/local/bin/dockerd-entrypoint.py"]
CMD ["serve"]
